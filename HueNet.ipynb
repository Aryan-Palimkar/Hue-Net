{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc440da0-23af-4078-bbe7-618f1543f7ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading from https://www.kaggle.com/api/v1/datasets/download/diraizel/anime-images-dataset?dataset_version_number=1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 868M/868M [06:37<00:00, 2.29MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: C:\\Users\\ARYAN PALIMKAR\\.cache\\kagglehub\\datasets\\diraizel\\anime-images-dataset\\versions\\1\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"diraizel/anime-images-dataset\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1eb860b-c070-4481-9fd5-4a3580972855",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(), \n",
    "    transforms.Normalize(mean=[0.5], std=[0.5]) \n",
    "])\n",
    "\n",
    "root_dir = \"C:/Users/ARYAN PALIMKAR/Desktop/Aryan/Jupyter/project/U-Net Upscaler/data\" \n",
    "dataset = datasets.ImageFolder(root=root_dir, transform=transform)\n",
    "dataset.samples = [(path, label) for path, label in dataset.samples if path.endswith(('.png', '.jpg'))]\n",
    "dataset.imgs = dataset.samples \n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=True, num_workers=4, pin_memory=True)\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "images, labels = next(data_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d9859a-3eeb-40d5-b9df-eb6924305b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torchvision.utils as vutils\n",
    "\n",
    "def show_images(images):\n",
    "    plt.figure(figsize=(32, 18))\n",
    "    plt.imshow(vutils.make_grid(images[:16], normalize=True, nrow=4).permute(1, 2, 0))\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1dff69e3-dc74-4722-981e-755496fd1a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_batch(dl, nmax=64):\n",
    "  for images, _ in dl:\n",
    "    show_images(images)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b080fe-b993-415f-8c5e-7936ec3ecba2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f264ee-d4ac-4c4d-9eba-c20a1a8827b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e336e5a7-fc9f-41d7-a138-a4493752585a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_default_device():\n",
    "  if torch.cuda.is_available():\n",
    "    return torch.device('cuda')\n",
    "  else:\n",
    "    return torch.device('cpu')\n",
    "\n",
    "def to_device(data, device):\n",
    "  if isinstance(data, (list,tuple)):\n",
    "    return [to_device(x, device) for x in data]\n",
    "  return data.to(device, non_blocking = True)\n",
    "\n",
    "class DeviceDataLoader():\n",
    "  def __init__(self, dl, device):\n",
    "    self.dl = dl\n",
    "    self.device = device\n",
    "\n",
    "  def __iter__(self):\n",
    "    for b in self.dl:\n",
    "      yield to_device(b, self.device)\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27efb8b4-5f8e-4107-af26-a611acccaeaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = get_default_device()\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "420f0682-ef37-4d2e-a5b9-85a510631c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DeviceDataLoader(dataloader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36eb03e3-9c47-43ee-a752-a1bf853bd461",
   "metadata": {},
   "source": [
    "# Generator using U-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1de1e718-c3d4-4945-9633-69aef8693616",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "\n",
    "class UnetGenerator(nn.Module):\n",
    "    def __init__(self, in_channels=1, out_channels=3):\n",
    "        super(UnetGenerator, self).__init__()\n",
    "    \n",
    "        # Encoder\n",
    "        self.enc1 = self.conv_block(in_channels, 64)\n",
    "        self.enc2 = self.conv_block(64, 128)\n",
    "        self.enc3 = self.conv_block(128, 256)\n",
    "\n",
    "        # Bottleneck\n",
    "        self.bottleneck = self.conv_block(256, 512)\n",
    "\n",
    "        # Decoder\n",
    "        self.upconv3 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
    "        self.dec3 = self.conv_block(512, 256)\n",
    "        self.upconv2 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "        self.dec2 = self.conv_block(256, 128)\n",
    "        self.upconv1 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "        self.dec1 = self.conv_block(128, 64)\n",
    "\n",
    "        # Output layer\n",
    "        self.out_conv = nn.Conv2d(64, out_channels, kernel_size=1)\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    # Conv block\n",
    "    def conv_block(self, in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        e1 = self.enc1(x)\n",
    "        e2 = self.enc2(F.max_pool2d(e1, 2))\n",
    "        e3 = self.enc3(F.max_pool2d(e2, 2))\n",
    "        b = self.bottleneck(F.max_pool2d(e3, 2))\n",
    "        d3 = self.upconv3(b)\n",
    "        d3 = torch.cat([d3, e3], dim=1)\n",
    "        d3 = self.dec3(d3)\n",
    "        d2 = self.upconv2(d3)\n",
    "        d2 = torch.cat([d2, e2], dim=1)\n",
    "        d2 = self.dec2(d2)\n",
    "        d1 = self.upconv1(d2)\n",
    "        d1 = torch.cat([d1, e1], dim=1)\n",
    "        d1 = self.dec1(d1)\n",
    "        out = self.out_conv(d1)\n",
    "        return self.tanh(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bb8639fd-dd88-44bb-b6c0-b62d787493e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = UnetGenerator()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752ade92-d3df-44ed-8212-55bcc31817d1",
   "metadata": {},
   "source": [
    "# Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aeba5737-7fbc-40cd-a1bf-6d34d7dbdb0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, in_channels=3):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        def discriminator_block(in_filters, out_filters, stride=2, normalize=True):\n",
    "            layers = [nn.Conv2d(in_filters, out_filters, kernel_size=4, stride=stride, padding=1)]\n",
    "            if normalize:\n",
    "                layers.append(nn.InstanceNorm2d(out_filters))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            return layers\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            *discriminator_block(in_channels, 64, normalize=False),\n",
    "            *discriminator_block(64, 128),\n",
    "            *discriminator_block(128, 256),\n",
    "            *discriminator_block(256, 512, stride=1),\n",
    "            nn.Conv2d(512, 1, kernel_size=4, stride=1, padding=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "923f22ae-7c57-472b-8fba-70ec6076b19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator = Discriminator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "249952e7-c5a7-4a1d-a4bf-d56d466116a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerceptualLoss(nn.Module):\n",
    "    def __init__(self, feature_layer=36):\n",
    "        super(PerceptualLoss, self).__init__()\n",
    "        vgg = models.vgg19(pretrained=True).features\n",
    "        self.vgg = nn.Sequential(*list(vgg.children())[:feature_layer]).eval()\n",
    "        for param in self.vgg.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        pred_features = self.vgg(pred)\n",
    "        target_features = self.vgg(target)\n",
    "        return F.mse_loss(pred_features, target_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44fea627-f4e8-41be-9172-0ca71675f273",
   "metadata": {},
   "outputs": [],
   "source": [
    "adversarial_loss = nn.BCEWithLogitsLoss()\n",
    "l1_loss = nn.L1Loss()\n",
    "perceptual_loss = PerceptualLoss().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217679f5-2aec-422e-93b5-f851d637f206",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d7e03933-c7c7-467e-8db2-16359ff13bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "g_optimizer = torch.optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "d_optimizer = torch.optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bf0e52a4-8a73-4dca-b92b-61cd804fe0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "grayscale_transform = transforms.Grayscale(num_output_channels=1)\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4b09c554-ae96-4071-9d82-5cfea4d9fb1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discriminator(\n",
       "  (model): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (3): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (6): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (7): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1))\n",
       "    (9): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (10): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (11): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator.to(device)\n",
    "discriminator.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c88802b-f6c8-4c11-a09f-75ee3d7fc354",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ARYAN PALIMKAR\\AppData\\Local\\Temp\\ipykernel_14456\\3548814718.py:8: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()  # For AMP\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms.functional as TF\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "scaler = GradScaler()\n",
    "\n",
    "def train(epochs):\n",
    "    for epoch in range(epochs):\n",
    "        epoch_g_loss, epoch_d_loss = 0.0, 0.0\n",
    "        num_batches = len(dataloader)\n",
    "\n",
    "        for images, _ in tqdm(dataloader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
    "            images = images.to(device, non_blocking=True)\n",
    "            grey = TF.rgb_to_grayscale(images, num_output_channels=1)\n",
    "\n",
    "            # --- Train Discriminator ---\n",
    "            d_optimizer.zero_grad()\n",
    "            with autocast():\n",
    "                real_output = discriminator(images)\n",
    "                fake_images = generator(grey)\n",
    "                fake_output = discriminator(fake_images.detach())\n",
    "\n",
    "                real_label = torch.ones_like(real_output).to(device)\n",
    "                fake_label = torch.zeros_like(fake_output).to(device)\n",
    "\n",
    "                d_real_loss = adversarial_loss(real_output, real_label)\n",
    "                d_fake_loss = adversarial_loss(fake_output, fake_label)\n",
    "\n",
    "                d_loss = (d_real_loss + d_fake_loss) / 2\n",
    "\n",
    "            scaler.scale(d_loss).backward()\n",
    "            scaler.step(d_optimizer)\n",
    "\n",
    "            # --- Train Generator ---\n",
    "            g_optimizer.zero_grad()\n",
    "            with autocast():\n",
    "                fake_output = discriminator(fake_images)\n",
    "                g_adv_loss = adversarial_loss(fake_output, real_label)\n",
    "                g_l1_loss = l1_loss(fake_images, images)\n",
    "\n",
    "                # Reduce perceptual loss resolution\n",
    "                small_fake = nn.functional.interpolate(fake_images, size=(128, 128), mode='bilinear')\n",
    "                small_real = nn.functional.interpolate(images, size=(128, 128), mode='bilinear')\n",
    "                g_perceptual_loss = perceptual_loss(normalize(small_fake), normalize(small_real))\n",
    "\n",
    "                g_loss = g_adv_loss + 100 * g_l1_loss + 0.1 * g_perceptual_loss\n",
    "\n",
    "            scaler.scale(g_loss).backward()\n",
    "            scaler.step(g_optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            epoch_g_loss += g_loss.item()\n",
    "            epoch_d_loss += d_loss.item()\n",
    "\n",
    "        epoch_g_loss /= num_batches\n",
    "        epoch_d_loss /= num_batches\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], G Loss: {epoch_g_loss:.4f}, D Loss: {epoch_d_loss:.4f}\")\n",
    "\n",
    "        \n",
    "        if epoch % 20 == 0:\n",
    "            with torch.no_grad():\n",
    "                plt.subplot(1, 3, 1)\n",
    "                plt.title(\"Grayscale Input\")\n",
    "                plt.imshow(grey[0, 0].cpu().numpy(), cmap='gray')\n",
    "        \n",
    "                # Ground Truth RGB\n",
    "                plt.subplot(1, 3, 2)\n",
    "                plt.title(\"Ground Truth\")\n",
    "                img_rgb = (images[0].permute(1, 2, 0).cpu().numpy() + 1) / 2\n",
    "                plt.imshow(np.clip(img_rgb, 0, 1))\n",
    "        \n",
    "                # Colorized Output\n",
    "                plt.subplot(1, 3, 3)\n",
    "                plt.title(\"Colorized Output\")\n",
    "                fake_img = (fake_images[0].detach().permute(1, 2, 0).cpu().numpy() + 1) / 2\n",
    "                plt.imshow(np.clip(fake_img, 0, 1))\n",
    "        \n",
    "                plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d8c436-b1a1-465f-a502-4f48a93c1495",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train(epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6171a08d-fb8c-4e17-8895-28fcf64d826f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520340c6-1287-458f-8186-db3cfdd01165",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7168063c-5b83-421d-823b-8e71065dfc6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torchvision.transforms.functional as TF\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.utils as vutils\n",
    "\n",
    "def test_model(images, generator):\n",
    "    images = images.to(device)\n",
    "    grey = TF.rgb_to_grayscale(images, num_output_channels=1)\n",
    "    fake_images = generator(grey)\n",
    "\n",
    "    for i in range(len(images)):\n",
    "        plt.figure(figsize=(15, 5))\n",
    "\n",
    "        # Grayscale Input\n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.title(\"Grayscale Input\")\n",
    "        plt.imshow(grey[i, 0].cpu().numpy(), cmap='gray')\n",
    "\n",
    "        # Ground Truth RGB\n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.title(\"Ground Truth RGB\")\n",
    "        img_rgb = (images[i].permute(1, 2, 0).cpu().numpy() + 1) / 2\n",
    "        plt.imshow(np.clip(img_rgb, 0, 1))\n",
    "\n",
    "        # Colorized Output\n",
    "        plt.subplot(1, 3, 3)\n",
    "        plt.title(\"Colorized Output\")\n",
    "        fake_img = (fake_images[i].detach().permute(1, 2, 0).cpu().numpy() + 1) / 2\n",
    "        plt.imshow(np.clip(fake_img, 0, 1))\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95017467-c181-42d3-89db-b2a991f80f9c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a7ac6a-1a46-49f1-b213-aa0f33012ca5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6e8acd-626c-4091-b1a7-7dcb4cd797db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "torch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
